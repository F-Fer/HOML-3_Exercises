{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26622abe-103c-4956-86e4-e6b7453402d4",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a524b-e8b9-4e53-83a4-f5ddb6d208e0",
   "metadata": {},
   "source": [
    "Apache SpamAssasin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baad3200-8a9d-4e7f-ae56-11efd75555aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1b949-8ef7-496e-8c05-bc74dfd9063e",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8d83ea-9f6d-49e7-9a46-240db293d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "def fetch_spam_data():\n",
    "    spam_root = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    ham_url = spam_root + \"20030228_easy_ham.tar.bz2\"\n",
    "    spam_url = spam_root + \"20030228_spam.tar.bz2\"\n",
    "\n",
    "    spam_path = Path() / \"..\" / \"data\" / \"spam\"\n",
    "    spam_path.mkdir(parents=True, exist_ok=True)\n",
    "    for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url),\n",
    "                                    (\"spam\", \"spam\", spam_url)):\n",
    "        if not (spam_path / dir_name).is_dir():\n",
    "            path = (spam_path / tar_name).with_suffix(\".tar.bz2\")\n",
    "            print(\"Downloading\", path)\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            tar_bz2_file = tarfile.open(path)\n",
    "            tar_bz2_file.extractall(path=spam_path)\n",
    "            tar_bz2_file.close()\n",
    "    return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07c6d75e-5a03-454c-b0f9-74e0b87c45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_dir, spam_dir = fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aecd07c-ea16-440c-83c0-233482108c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spam_data(directory):\n",
    "    emails = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"latin1\") as email_file:\n",
    "                    emails.append(email_file.read())\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6346ed49-1dad-472d-a988-245f21a223fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam emails: 501\n",
      "Number of ham emails: 2502\n"
     ]
    }
   ],
   "source": [
    "ham_data = load_spam_data(ham_dir)\n",
    "spam_data = load_spam_data(spam_dir)\n",
    "\n",
    "print(f\"Number of spam emails: {len(spam_data)}\")\n",
    "print(f\"Number of ham emails: {len(ham_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ef7853-d4f6-40f3-aefa-ffa90d840858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ham_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb2fb0d-d7fb-4895-955f-0d3177edf3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>From spamassassin-commits-admin@lists.sourcefo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>From andijeanehuaa@yahoo.com  Wed Aug 28 10:43...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>From timc@2ubh.com  Wed Aug 28 10:54:56 2002\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>From tps@insiq.us  Mon Sep 16 00:27:17 2002\\nR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>From fork-admin@xent.com  Tue Aug 27 17:34:36 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>From fork-admin@xent.com  Thu Sep 19 11:04:59 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>From rssfeeds@jmason.org  Thu Sep 26 16:31:16 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>From tony@svanstrom.com  Fri Aug 23 11:05:51 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>From arnoldm@aol.com  Mon Sep  9 19:31:32 2002...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>From fork-admin@xent.com  Wed Sep 11 14:21:29 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Email  Label\n",
       "1329  From spamassassin-commits-admin@lists.sourcefo...      0\n",
       "2633  From andijeanehuaa@yahoo.com  Wed Aug 28 10:43...      1\n",
       "181   From timc@2ubh.com  Wed Aug 28 10:54:56 2002\\n...      0\n",
       "2810  From tps@insiq.us  Mon Sep 16 00:27:17 2002\\nR...      1\n",
       "343   From fork-admin@xent.com  Tue Aug 27 17:34:36 ...      0\n",
       "627   From fork-admin@xent.com  Thu Sep 19 11:04:59 ...      0\n",
       "1926  From rssfeeds@jmason.org  Thu Sep 26 16:31:16 ...      0\n",
       "929   From tony@svanstrom.com  Fri Aug 23 11:05:51 2...      0\n",
       "2755  From arnoldm@aol.com  Mon Sep  9 19:31:32 2002...      1\n",
       "555   From fork-admin@xent.com  Wed Sep 11 14:21:29 ...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_email_dataframe(emails, label):\n",
    "    df = pd.DataFrame({\n",
    "        \"Email\": emails,\n",
    "        \"Label\": label\n",
    "    })\n",
    "    return df\n",
    "\n",
    "ham_df = create_email_dataframe(ham_data,  0)\n",
    "spam_df = create_email_dataframe(spam_data, 1)\n",
    "\n",
    "email_df = pd.concat([ham_df, spam_df], ignore_index=True)\n",
    "\n",
    "email_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58714ab8-684d-41ac-9bd0-4f8654b3c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range (2500, 2511):\n",
    "    print(email_df.loc[i, \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cdfb28-3e7a-41d0-8b8b-9a6abcfd883e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3003, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffeling the data\n",
    "\n",
    "email_df = email_df.sample(frac=1, random_state=42)\n",
    "email_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652a1581-f363-44fe-8c3b-d92a24c803ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>From rssfeeds@jmason.org  Thu Oct  3 12:24:36 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>From 107664.1420@actionsports.co.uk  Sat Sep 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>From glynn.clements@virgin.net  Wed Sep  4 18:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>From ilug-admin@linux.ie  Fri Sep  6 11:40:19 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>From fork-admin@xent.com  Mon Sep 30 13:52:43 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>Return-Path: guido@python.org\\nDelivery-Date: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>From rpm-list-admin@freshrpms.net  Mon Sep  9 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>From exmh-workers-admin@redhat.com  Mon Sep 23...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>From rpm-list-admin@freshrpms.net  Tue Oct  8 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>From fork-admin@xent.com  Thu Oct  3 12:54:57 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3003 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Email  Label\n",
       "2211  From rssfeeds@jmason.org  Thu Oct  3 12:24:36 ...      0\n",
       "2905  From 107664.1420@actionsports.co.uk  Sat Sep 2...      1\n",
       "1411  From glynn.clements@virgin.net  Wed Sep  4 18:...      0\n",
       "251   From ilug-admin@linux.ie  Fri Sep  6 11:40:19 ...      0\n",
       "794   From fork-admin@xent.com  Mon Sep 30 13:52:43 ...      0\n",
       "...                                                 ...    ...\n",
       "1638  Return-Path: guido@python.org\\nDelivery-Date: ...      0\n",
       "1095  From rpm-list-admin@freshrpms.net  Mon Sep  9 ...      0\n",
       "1130  From exmh-workers-admin@redhat.com  Mon Sep 23...      0\n",
       "1294  From rpm-list-admin@freshrpms.net  Tue Oct  8 ...      0\n",
       "860   From fork-admin@xent.com  Thu Oct  3 12:54:57 ...      0\n",
       "\n",
       "[3003 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8518135-8305-414d-98e4-2d6369a34c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (3003,)\n",
      "Shape of y: (3003,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into X and y\n",
    "\n",
    "X = email_df.loc[:, \"Email\"]\n",
    "y = email_df.loc[:, \"Label\"]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72400b3f-9c88-4997-b628-e5d6e2dbbf38",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bfaec3-cd21-41d9-9359-5bbe89ba1f5b",
   "metadata": {},
   "source": [
    "*Preprocessing steps:*\n",
    "- lower casing\n",
    "- removal of punctuations\n",
    "- removal of stopwords\n",
    "- removal of frequent words\n",
    "- stemming\n",
    "- lemmatisation\n",
    "- conversion of emoticons to words\n",
    "- removal of words\n",
    "- removal of html tags\n",
    "- spelling correction\n",
    "- removal of rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88755ceb-bc10-4c6e-b218-11ee57f8d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower casing\n",
    "\n",
    "X_lower = X.apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8760ccb-061f-4f21-bf72-80fb002e343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of punctuation\n",
    "# string.puntuation = {!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~}\n",
    "\n",
    "import string \n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else \" \" for ch in text])\n",
    "\n",
    "X_lower_nopunc = X_lower.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97c26715-b8bf-41e3-8911-108f3e165942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Finn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# removal of stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenization(text):\n",
    "    tk = WhitespaceTokenizer()\n",
    "    return tk.tokenize(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [token for token in text if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39e16ecb-327e-4ea6-b57a-0eb3392ca262",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower_nopunc_token = X_lower_nopunc.apply(tokenization)\n",
    "X_lower_nopunc_token_nostopw = X_lower_nopunc_token.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aadb8d8f-befa-47c0-83b0-07caf686c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of frequent words. Alternative to removing stopwords.\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in X_lower_nopunc_token:\n",
    "    for word in text:\n",
    "        cnt[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed51af77-5c19-4914-aca6-a60370502c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = set([w for (w, wc) in cnt.most_common(20)])\n",
    "\n",
    "def remove_frequent_words(text):\n",
    "    return [token for token in text if token not in frequent_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70a08f17-3930-4855-9d5c-ce249ea68d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower_nopunc_token_nofreqw = X_lower_nopunc_token.apply(remove_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7e66867-09ac-46d9-ac7c-a32da1f1e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of rare words\n",
    "\n",
    "n_rare_words = 200\n",
    "rare_words = set([w for (w, wc) in cnt.most_common()[len(cnt.most_common())-n_rare_words:]])\n",
    "\n",
    "def remove_rare_words(text):\n",
    "    return [word for word in text if word not in rare_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41545d86-9ff4-43de-9d77-5f4723ca7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower_nopunc_token_nofreqw_norarew = X_lower_nopunc_token_nofreqw.apply(remove_rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e85736a-04b3-4958-b51f-c8ac3a436a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return [stemmer.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d92a6e4d-b289-4357-a040-ed51039de7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happier', 'dog', 'bye', 'misser', 'run']\n"
     ]
    }
   ],
   "source": [
    "text = [\"happier\", \"dog\", \"bye\", \"misserable\", \"running\"]\n",
    "print(stem_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c54ce325-5e40-4ca6-a51a-85cf6e8f3b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0f19a48-3bca-4da8-9c94-dc63b93a2af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happier', 'dog', 'best', 'source', 'running']\n"
     ]
    }
   ],
   "source": [
    "text = [\"happier\", \"dogs\", \"best\", \"sources\", \"running\"]\n",
    "print(lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875762b-abcd-4034-bcc1-378b5e5d4d68",
   "metadata": {},
   "source": [
    "**Preprocessing Pipeline Order**\n",
    "1. Lower casing\n",
    "2. Replacing URLs with \"URL\"\n",
    "3. Replacing numbers with \"Number\"\n",
    "4. Replacing emoticons with words\n",
    "5. Removal of html tags\n",
    "6. Removal of punctuation\n",
    "7. Tokenization\n",
    "8. Lemmatization\n",
    "9. Removal of frequent words\n",
    "10. Removal of rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "47dfd8ec-e391-4697-bd1f-0e6962e51582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def replace_urls(text):\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(url_pattern, 'URL', text)\n",
    "\n",
    "def replace_numbers(text):\n",
    "    number_pattern = r'\\d+'\n",
    "    return re.sub(number_pattern, 'NUMBER', text)\n",
    "\n",
    "def replace_emoticons(text):\n",
    "    emoticons = {\n",
    "        ':)': 'smile',\n",
    "        ':(': 'sad',\n",
    "        ':D': 'grin',\n",
    "        ':P': 'playful',\n",
    "        ':-)': 'smile',\n",
    "        ':-(': 'sad',\n",
    "    }\n",
    "    pattern = re.compile('|'.join(re.escape(emoticon) for emoticon in emoticons.keys()))\n",
    "    return pattern.sub(lambda match: emoticons[match.group(0)], text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return \"\".join([ch if ch not in string.punctuation else \" \" for ch in text])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def preprocess_email(email:str):\n",
    "    text = email.lower() # lower casing\n",
    "    text = replace_urls(text) # replacing URLs\n",
    "    text = replace_numbers(text) # replacing numbers \n",
    "    text = replace_emoticons(text) # replacing emoticons\n",
    "    text = remove_html_tags(text) # removing html tags\n",
    "    text = remove_punctuation(text) # removing punctuation\n",
    "    tokens = text.split(\" \") # tokenization with \" \"\n",
    "    tokens = lemmatize_words(tokens) # lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "acfd5e17-c3e0-4001-855f-1b970586e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformer for removing frequent and rare words\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in X_lower_nopunc_token:\n",
    "    for word in text:\n",
    "        cnt[word] += 1\n",
    "\n",
    "class FreqRareWordRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_frequent_words:int, n_rare_words:int):\n",
    "        self.n_frequent_words = n_frequent_words\n",
    "        self.n_rare_words = n_rare_words\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = check_array(X, dtype=object)\n",
    "        self.cnt_ = Counter()\n",
    "        for _, text in X.iterrows():\n",
    "            for token in text:\n",
    "                self.cnt_[token] += 1\n",
    "        self.FREQUENT_WORDS_ = set([w for (w, wc) in cnt.most_common(self.n_frequent_words)])\n",
    "        self.RARE_WORDS_ = set([w for (w, wc) in cnt.most_common()[len(self.cnt_.most_common())-self.n_rare_words:]])\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X, dtype=object)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        filtered_texts = []\n",
    "        for _, text in X.iterrows():\n",
    "            filtered_row = [word for word in text if (word not in self.FREQUENT_WORDS_ and word not in self.RARE_WORDS_)]\n",
    "            filtered_texts.append(filtered_row)\n",
    "        return pd.DataFrame(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "89f0a957-65b0-4f62-ab96-be0cc3f9bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "preprocessing = Pipeline([\n",
    "    (\"basic_preprocessing\", FunctionTransformer(func=lambda X: X.apply(preprocess_email))),\n",
    "    (\"freqRareWordRomover\", FreqRareWordRemover(20, 200))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21a134fd-9c8f-4a0e-92a0-e8cae39e9e22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[None None None ... None None None].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessed_emails \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\pipeline.py:479\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    477\u001b[0m fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit_transform(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\u001b[38;5;241m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[1;32mIn[105], line 17\u001b[0m, in \u001b[0;36mFreqRareWordRemover.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 17\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnt_ \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, text \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39miterrows():\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\homl3\\lib\\site-packages\\sklearn\\utils\\validation.py:938\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    943\u001b[0m         )\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[None None None ... None None None].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "preprocessed_emails = preprocessing.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d9fc2-e896-4ae6-a16d-1a3222785665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
