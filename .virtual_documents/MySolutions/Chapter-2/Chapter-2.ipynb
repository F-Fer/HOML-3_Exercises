import sys

assert sys.version_info >= (3, 7)











from pathlib import Path
import pandas as pd
import tarfile
import urllib.request
import numpy as np


def load_housing_data():
    """
    A tarball is a set of files packaged together into a single file, 
    then compressed using the gzip compression program.
    """
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
    # returns a Pandas DataFrame
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data()





housing.info()


housing.head()


housing["ocean_proximity"].value_counts()


housing.shape


housing["ocean_proximity"][900:1000]


housing.describe()





IMAGES_PATH = Path() / "images" / "end_to_end_project"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)


import matplotlib.pyplot as plt

# extra code â€“ the next 5 lines define the default font sizes
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

housing.hist(bins=50, figsize=(12, 8))
save_fig("attribute_histogram_plots")  # save the figure
plt.show()





def shuffle_and_split_data(data, test_ratio):
    np.random.seed(42)
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]


train_set, test_set = shuffle_and_split_data(housing, 0.2)





# Making a copy of the train set
housing = train_set.copy()





housing.plot(kind="scatter", x="longitude", y="latitude", grid=True, alpha=0.2)
plt.show()


housing.plot(kind="scatter", x="longitude", y="latitude", grid=True, s=housing["population"]/100, 
             label="population", c="median_house_value", cmap="jet", colorbar=True, legend=True, sharex=False, figsize=(10, 7))
plt.show()





# corr() computes the standard correlation coefficient between every pair of attributes.
# Ranges from -1 to +1
# Only do this computation if the dataset is not too large
# Only captures linear correlations!

corr_matrix = housing.corr(numeric_only=True)


corr_matrix["median_house_value"].sort_values(ascending=False)


# Pandas scatter_matrix() function plots every numerical attribute against every other numerical attribute 

from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(16, 10))
plt.show()








housing_labels = train_set["median_house_value"].copy()
housing = train_set.drop("median_house_value", axis=1)








# The total_bedrooms attribute has some missing values.
# I am fixing this by inserting the mean of the attribute into the missing values (imputation)

median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)


# Better version of the preceding code is the Scikit-Learn class SimpleImputer.

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")

# Create version of housing dataset with only numerical attributes
housing_numeric = housing.select_dtypes(include=[np.number])

imputer.fit(housing_numeric)


# The Imputer saves the meadian of all attributes in its statistics_ instance variable.
imputer.statistics_


# Now using the transform() method will replace all missing values with the norm of its attribute
X = imputer.transform(housing_numeric)





# the ocean_proximity attribute contains text data

housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)


# Converting the text to category-ints

from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)


housing_cat_encoded[:10]


# A list of the corresponding categories is saved in the categories_ instance variable

ordinal_encoder.categories_


# The issue with this representation is, that ML algorythms assume that two close values are more similar than two distant ones.
# This may be fine for ordered categories (such as bad, average, good), but this is not the case for ocean_proximity.
# A better approach is to use the scikit-learn's OneHotEncoder

from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_onehot = cat_encoder.fit_transform(housing_cat)


# By default, the output of the OneHotEncoder is a SciPy sparse matrix instead of a NumPy array.

housing_cat_onehot


housing_cat_onehot.toarray()


housing_cat_onehot.shape


# The categories are now saved in the OneHotEncoders categories_ instance variable

cat_encoder.categories_





# Min-Max Scaling (also called Normalization)

from sklearn.preprocessing import MinMaxScaler

minmax_scaler = MinMaxScaler(feature_range=(-1, 1))
housing_num_minmax_scaled = minmax_scaler.fit_transform(housing_numeric)


# Standartization

from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
housing_num_std_scaled = std_scaler.fit_transform(housing_numeric)


# Long tail attributes

fig, axs = plt.subplots(figsize=(8, 3), sharey=True)
housing["population"].hist(bins=50)
axs.set_xlabel("Population")
axs.set_ylabel("Number of districts")
plt.show()


# Log-Transformer to scale the population attribute

from sklearn.preprocessing import FunctionTransformer

log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
log_population = log_transformer.transform(housing[["population"]])





from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.cluster import KMeans

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, n_init=10,
                              random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self  # always return self!

    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    
    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]





from sklearn.pipeline import make_pipeline

# Replacing missing values & standartization
num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler()) 

# Replacing missing values & 1Hot encoding the categories
cat_pipeline = make_pipeline(SimpleImputer(strategy="most_frequent"), OneHotEncoder(handle_unknown="ignore"))


# Using different transformation-pipelines for differnt types of attributes (columns).
from sklearn.compose import make_column_selector, make_column_transformer

preprocessing = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object))
)


housing_prepared = preprocessing.fit_transform(housing)


from sklearn.metrics.pairwise import rbf_kernel

def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

def ratio_name(funtion_transformer, feature_names_in):
    return ["ratio"]

def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler())

log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler())
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())


from sklearn.compose import ColumnTransformer

preprocessing = ColumnTransformer([
        ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
        ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
        ("people_per_house", ratio_pipeline(), ["population", "households"]),
        ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population",
                               "households", "median_income"]),
        ("geo", cluster_simil, ["latitude", "longitude"]),
        ("cat", cat_pipeline, make_column_selector(dtype_include=object)),
    ],
    remainder=default_num_pipeline)  # one column remaining: housing_median_age


housing_prepared = preprocessing.fit_transform(housing)
housing_prepared.shape 
# Added 7 extra columns 
# (5 from OneHot encoding the categorical attributes & 


housing.shape


preprocessing.get_feature_names_out()


preprocessing.output_indices_








from sklearn.linear_model import LinearRegression

lin_reg = make_pipeline(preprocessing, LinearRegression())
lin_reg.fit(housing, housing_labels)





housing_predictions = lin_reg.predict(housing)
print(f"Prediction: \t\t{housing_predictions[:5].round(-2)}") # -2 rounded to the nearest hundred
print(f"Actual Valuation: \t{housing_labels.iloc[:5].values}")








from sklearn.metrics import mean_squared_error

lin_reg_mse = mean_squared_error(housing_labels, housing_predictions, squared=False) # With squared to False it is RMSE and not MSE

lin_reg_mse





from sklearn.tree import DecisionTreeRegressor

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
tree_reg.fit(housing, housing_labels)








housing_predictions = tree_reg.predict(housing)

tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False) # With squared to False it is RMSE and not MSE

tree_rmse








from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)

pd.Series(tree_rmses).describe()





from sklearn.ensemble import RandomForestRegressor

forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))
forest_reg.fit(housing, housing_labels)











from sklearn.model_selection import cross_val_score

forest_rmses = -cross_val_score(forest_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)

pd.Series(forest_rmses).describe()








# GridSearchCV already includes cross validation

from sklearn.model_selection import GridSearchCV 
from sklearn.pipeline import Pipeline

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],
     'random_forest__max_features': [4, 6, 8]},
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]

grid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring="neg_root_mean_squared_error")
grid_search.fit(housing, housing_labels)


grid_search.best_params_


grid_search.cv_results_


cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res.head()





from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),
                  'random_forest__max_features': randint(low=2, high=20)}

rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, 
    n_iter=10, cv=3, scoring='neg_root_mean_squared_error', random_state=42)

rnd_search.fit(housing, housing_labels)








final_model = rnd_search.best_estimator_ # Returns a pipeline object
# "random_forest" was the name defined to the randomForest model inside the pipeline
feature_importances = final_model["random_forest"].feature_importances_ 
feature_importances.round(2)





sorted(
    zip(feature_importances, final_model['preprocessing'].get_feature_names_out()), 
    reverse=True)


len(
    sorted(
        zip(feature_importances, final_model['preprocessing'].get_feature_names_out()), 
        reverse=True))





X_test = test_set.drop("median_house_value", axis=1)
y_test = test_set["median_house_value"].copy()

final_predictions = final_model.predict(X_test)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)





from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors)-1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))


squared_errors.head()





import joblib

joblib.dump(final_model, "my_california_housing_regressor.plk")











from sklearn.svm import SVR


housing.shape


# Using only the first 5000 instances in the dataset, as SVMs dont scale well to large datasets
housing_small = housing[:5000]


housing_small.shape


housing_small.head()


X_housing_small = housing_small.drop(["median_house_value"], axis=1)
y_housing_small = housing_small["median_house_value"]
print(X_housing_small.shape)
print(y_housing_small.shape)


y_housing_small.head()


 X_housing_small.head()


# Preprocessign pipeline
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_selector, make_column_transformer
import numpy as np

num_preprocessing = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

cat_preprocessing = make_pipeline(SimpleImputer(strategy="most_frequent"), OneHotEncoder( handle_unknown="ignore"))

preprocessing = make_column_transformer(
    (num_preprocessing, make_column_selector(dtype_include=np.number)),
    (cat_preprocessing, make_column_selector(dtype_include=object))
)


model = Pipeline([
    ("preprocessing", preprocessing),
    ("svm", SVR())
])


# Grid Search Cross Validation

from sklearn.model_selection import GridSearchCV
from scipy.stats import randint

# This will train a model 20 times (4 + 4 * 4)
param_grid = [
    {
    "svm__kernel": ["linear"],
    "svm__C": [0.1, 1, 10, 100],
    }, {
    "svm__kernel": ["rbf"],
    "svm__C": [0.1, 1, 10, 100],
    "svm__gamma": [1e-3, 1e-4, 'scale', 'auto']
    }]

grid_search = GridSearchCV(model, param_grid=param_grid, scoring="neg_root_mean_squared_error", cv=3)


grid_search.fit(X_housing_small, y_housing_small)


# Looking at the best results

grid_search.best_params_


cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res.head(-1)


best_model = grid_search.best_estimator_


# Creating Test Set

housing_small_test = housing[5000:]

housing_small_test.shape


X_housing_small_test = housing_small_test.drop(["median_house_value"], axis=1)
y_housing_small_test = housing_small_test["median_house_value"].copy()
print(X_housing_small_test.shape)
print(y_housing_small_test.shape)





# Root mean squared error
from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(predictions, y_housing_small_test, squared=False)
print(rmse)








# Randomized Search Cross Validation

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

param_distribs = [
    {
    "svm__kernel": ["linear"],
    "svm__C": uniform(0, 100),
    }, {
    "svm__kernel": ["rbf"],
    "svm__C": uniform(0, 100),
    "svm__gamma": [1e-3, 1e-4, 'scale', 'auto']
    }]

random_search = RandomizedSearchCV(model, param_distributions=param_distribs, scoring="neg_root_mean_squared_error", n_iter=20, cv=3, random_state=42)


random_search.fit(X_housing_small, y_housing_small)


best_model = random_search.best_estimator_


print(random_search.best_params_)


# Predictions 

predictions = best_model.predict(X_housing_small_test)


rmse = mean_squared_error(predictions, y_housing_small_test, squared=False)
print(rmse)





from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestRegressor

# Random Forest Regressor for determiting the feature importances
random_forest = RandomForestRegressor(n_estimators=100, random_state=42)

model = Pipeline([
    ("preprocessing", preprocessing),
    ("selectFromModel", SelectFromModel(estimator=random_forest, threshold="median")),
    ("svm", SVR())
])





param_distribs = [
    {
    "svm__kernel": ["linear"],
    "svm__C": uniform(0, 100),
    }, {
    "svm__kernel": ["rbf"],
    "svm__C": uniform(0, 100),
    "svm__gamma": [1e-3, 1e-4, 'scale', 'auto']
    }]

random_search = RandomizedSearchCV(model, param_distributions=param_distribs, scoring="neg_root_mean_squared_error", n_iter=20, cv=3, random_state=42)

random_search.fit(X_housing_small, y_housing_small)

best_model = random_search.best_estimator_

print(random_search.best_params_)

predictions = best_model.predict(X_housing_small_test)

rmse = mean_squared_error(predictions, y_housing_small_test, squared=False)
print(rmse)








from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.neighbors import KNeighborsRegressor
from scipy.sparse import issparse

class KNNTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, n_neighbors=5):
        self.n_neighbors = n_neighbors

    def fit(self, X, y):
        if X is None or y is None:
            raise ValueError("X or y cannot be None")

        # Check for sparse input
        if issparse(X):
            raise ValueError("Sparse input is not supported.")

        X_nparr = np.asarray(X)
        if X_nparr.ndim == 1:
            raise ValueError("Input X must be 2D, but got 1D input.")
        
        self.knn_regressor_ = KNeighborsRegressor(n_neighbors=self.n_neighbors)
        self.knn_regressor_.fit(X, y)
        # Store the number of features seen during fit
        self.n_features_in_ = X_nparr.shape[1]
        return self

    def transform(self, X, y=None):
        predictions = self.knn_regressor_.predict(X)
        if predictions.ndim == 1:
            predictions = predictions.reshape(-1, 1)
        return predictions


# Trying it out
knn_transformer = KNNTransformer()
knn_transformer.fit(X_housing_small[["longitude", "latitude"]], y_housing_small)
knn_transformer.transform(X_housing_small_test[["longitude", "latitude"]]).shape


from sklearn.utils.estimator_checks import check_estimator

check_estimator(KNNTransformer())


# Buildiung the pipeline

from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import make_column_selector, ColumnTransformer
import numpy as np
from sklearn.svm import SVR

num_preprocessing = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

cat_preprocessing = make_pipeline(SimpleImputer(strategy="most_frequent"), OneHotEncoder( handle_unknown="ignore"))

preprocessing = ColumnTransformer([
    ("num_preprocessing", num_preprocessing, make_column_selector(dtype_include=np.number)),
    ("cat_preprocessing", cat_preprocessing, make_column_selector(dtype_include=object)),
    ("knn_transformer", KNNTransformer(n_neighbors=5), ["latitude", "longitude"])]
)

model = Pipeline([
    ("preprocessing", preprocessing),
    ("svm", SVR())
])


# Trying it without hyper parameter tuning
best_C = random_search.best_params_["svm__C"]
best_gamma = random_search.best_params_["svm__gamma"]
best_kernel = random_search.best_params_["svm__kernel"]

model.set_params([
    ("svm__C": best_C),
    ("svm__gamma": best_gamma),
    ("svm__kernel": best_kernel)])


model.fit(X_housing_small, y_housing_small)


param_distribs = [
    {
    "svm__kernel": ["linear"],
    "svm__C": uniform(0, 100),
    }, {
    "svm__kernel": ["rbf"],
    "svm__C": uniform(0, 100),
    "svm__gamma": [1e-3, 1e-4, 'scale', 'auto']
    }]

random_search = RandomizedSearchCV(model, param_distributions=param_distribs, scoring="neg_root_mean_squared_error", n_iter=5, cv=3, random_state=42)

random_search.fit(X_housing_small, y_housing_small)

best_model = random_search.best_estimator_

print(random_search.best_params_)

predictions = best_model.predict(X_housing_small_test)

rmse = mean_squared_error(predictions, y_housing_small_test, squared=False)
print(rmse)





from sklearn.base import BaseEstimator, TransformerMixin

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        pass

    def transform(self, X, y=None):
        pass

    def 
